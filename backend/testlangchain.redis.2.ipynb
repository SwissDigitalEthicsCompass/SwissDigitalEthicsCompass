{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "local_llm = \"aiden_lu/minicpm-v2.6:Q4_K_M\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# Define the directory where PDF files are stored\n",
    "pdf_dir = \"backend/data_RAG\"\n",
    "output_dir = \"backend/extracted_images_pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain import hub\n",
    "#from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "#from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from chromadb.config import Settings\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import chromadb\n",
    "chromadb.api.client.SharedSystemClient.clear_system_cache()\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "from langchain_community.storage import RedisStore\n",
    "from langchain_community.utilities.redis import get_client\n",
    "\n",
    "client = chromadb.PersistentClient(\n",
    "#        path=\"backend/chroma\",  # commented this to not to overwrite\n",
    "        settings=Settings(allow_reset=True),\n",
    "        tenant=DEFAULT_TENANT,\n",
    "        database=DEFAULT_DATABASE,\n",
    "        )\n",
    "\n",
    "#client.reset()  # resets the database\n",
    "#collection = client.get_or_create_collection(\"test_my_rag\")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"test_my_rag\",\n",
    "#    documents=metafiltered_docs,\n",
    "    embedding_function=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    "    persist_directory=\"backend/chroma_langchain_rag_db_test\",  # Optional: directory to store the Chroma database\n",
    "    client_settings=Settings(anonymized_telemetry=False, allow_reset=True),\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "# Initialize the storage layer - to store raw images, text and tables\n",
    "redis_url=\"redis://:mypassword@localhost:6379/0\"\n",
    "\n",
    "redis_client = get_client(redis_url)\n",
    "redis_store = RedisStore(client=redis_client) # you can use filestore, memorystore, any other DB store also\n",
    "\n",
    "#byte_store = InMemoryByteStore()\n",
    "\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# Setup MultiVectorRetriever with byte_store\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=redis_store,\n",
    "#    byte_store=redis_store,\n",
    "    id_key=id_key,\n",
    "#    search_type=\"similarity\",  # Specify the search type as needed\n",
    "#    k=3  # Number of results to retrieve\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from IPython.display import HTML, display, Image\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Decode the base64 string\n",
    "    img_data = base64.b64decode(img_base64)\n",
    "    # Create a BytesIO object\n",
    "    img_buffer = BytesIO(img_data)\n",
    "    # Open the image using PIL\n",
    "    img = Image.open(img_buffer)\n",
    "    display(img)\n",
    "\n",
    "# helps in detecting base64 encoded strings\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "# helps in checking if the base64 encoded image is actually an image\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# returns a dictionary separating images and text (with table) elements\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts (with tables)\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content.decode('utf-8')\n",
    "        else:\n",
    "            doc = doc.decode('utf-8')\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check retrieval\n",
    "query = \"How many questions in survey results got consensus and how many got dissensus?\"\n",
    "docs = retriever.invoke(query, limit=5)\n",
    "docstores_docs = retriever.vectorstore.similarity_search(\"How many questions in survey results got consensus and how many got dissensus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "import htmltabletomd\n",
    "\n",
    "def multimodal_prompt_function(data_dict):\n",
    "    \"\"\"\n",
    "    Create a multimodal prompt with both text and image context.\n",
    "    This function formats the provided context from `data_dict`, which contains\n",
    "    text, tables, and base64-encoded images. It joins the text (with table) portions\n",
    "    and prepares the image(s) in a base64-encoded format to be included in a \n",
    "    message.\n",
    "    The formatted text and images (context) along with the user question are used to\n",
    "    construct a prompt for GPT-4o\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "    \n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "    \n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            f\"\"\"You are an analyst tasked with understanding detailed information \n",
    "                from text documents, data tables, and charts and graphs in images.\n",
    "                You will be given context information below which will be a mix of \n",
    "                text, tables, and images usually of charts or graphs.\n",
    "                Use this information to provide answers related to the user \n",
    "                question.\n",
    "                Do not make up answers, use the provided context documents below and \n",
    "                answer the question to the best of your ability.\n",
    "                \n",
    "                User question:\n",
    "                {data_dict['question']}\n",
    "                \n",
    "                Context documents:\n",
    "                {formatted_texts}\n",
    "                \n",
    "                Answer:\n",
    "            \"\"\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain\n",
    "multimodal_rag = (\n",
    "        {\n",
    "            \"context\": itemgetter('context'),\n",
    "            \"question\": itemgetter('input'),\n",
    "        }\n",
    "            |\n",
    "        RunnableLambda(multimodal_prompt_function)\n",
    "            |\n",
    "        llm\n",
    "            |\n",
    "        StrOutputParser()\n",
    ")\n",
    "\n",
    "# Pass input query to retriever and get context document elements\n",
    "retrieve_docs = (itemgetter('input')\n",
    "                    |\n",
    "                retriever\n",
    "                    |\n",
    "                RunnableLambda(split_image_text_types))\n",
    "\n",
    "# Below, we chain `.assign` calls. This takes a dict and successively\n",
    "# adds keys-- \"context\" and \"answer\"-- where the value for each key\n",
    "# is determined by a Runnable (function or chain executing at runtime).\n",
    "# This helps in having the retrieved context along with the answer generated by MLLM\n",
    "multimodal_rag_w_sources = (RunnablePassthrough.assign(context=retrieve_docs)\n",
    "                                               .assign(answer=multimodal_rag)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "def multimodal_rag_qa(query):\n",
    "    response = multimodal_rag_w_sources.invoke({'input': query})\n",
    "    print('=='*50)\n",
    "    print('Answer:')\n",
    "    display(Markdown(response['answer']))\n",
    "    print('--'*50)\n",
    "    print('Sources:')\n",
    "    text_sources = response['context']['texts']\n",
    "    img_sources = response['context']['images']\n",
    "    for text in text_sources:\n",
    "        display(Markdown(text))\n",
    "        print()\n",
    "    for img in img_sources:\n",
    "        plt_img_base64(img)\n",
    "        print()\n",
    "    print('=='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query = \"Explain each layer in the theoretical framework and show the corresponding figure as image\"\n",
    "\n",
    "query = \"Can you explain in details the steps, that used in the customozed Delphi method\"\n",
    "\n",
    "query = \"How many questions in survey results got consensus and how many got dissensus?\" # Cannot answer\n",
    "\n",
    "query = \"How many answers in survey results got consensus?\" #Wrong or not correct answer\n",
    "\n",
    "query = \"Summary the article selection process and give exact number of articles found in each library using PRISMA framework. Show table!\" \n",
    "\n",
    "query = \"What is the difference in the number of articles categorized under “Service” between IEEEXplore and ACM?\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many questions in survey results got consensus and how many got dissensus? Show table data in table format\" \n",
    "multimodal_rag_qa(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
